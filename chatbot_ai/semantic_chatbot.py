import os
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import unicodedata
import re
import requests

DATA_FILE = "data_chatbot.json"
EMBEDDING_FILE = "nganh_embeddings.npy"
INDEX_FILE = "nganh_faiss.index"
ID_FILE = "nganh_ids.json"
MODEL_NAME = "all-MiniLM-L6-v2"

# Alias mapping: alias (kh√¥ng d·∫•u, vi·∫øt th∆∞·ªùng) -> m√£ ng√†nh
ALIAS_MAP = {
    'cntt': '7480201',
    'it': '7480201',
    'cong nghe thong tin': '7480201',
    'khmt': '7480101',
    'khoa hoc may tinh': '7480101',
    'ktpm': '7480103',
    'ky thuat phan mem': '7480103',
    'mmt': '7480102',
    'mang may tinh': '7480102',
    'ttnt': '7480107',
    'tri tue nhan tao': '7480107',
    # Th√™m alias cho c√°c ng√†nh kh√°c n·∫øu mu·ªën
}

# Tham s·ªë RAG
TOP_K = 3  # S·ªë ng√†nh l·∫•y l√†m context cho LLM

# Helper: remove accents for better matching
def remove_accents(input_str):
    nfkd_form = unicodedata.normalize('NFKD', input_str)
    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)]).lower()

def load_nganh_data():
    if not os.path.exists(DATA_FILE):
        print(f"[ERROR] File d·ªØ li·ªáu {DATA_FILE} kh√¥ng t·ªìn t·∫°i!")
        exit(1)
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['nganhs']

def get_nganh_by_ma(ma_nganh, nganhs):
    for ng in nganhs:
        if ng.get('ma_nganh') == ma_nganh:
            return ng
    return None

def build_embeddings_and_index(nganhs, model):
    texts = []
    ids = []
    for idx, ng in enumerate(nganhs):
        # Gh√©p t√™n + m√¥ t·∫£ + m√£ ng√†nh + alias
        alias_text = ''
        for alias, ma_nganh in ALIAS_MAP.items():
            if ng.get('ma_nganh') == ma_nganh:
                alias_text += f" {alias.upper()} {alias.lower()}"
        text = f"{ng.get('nganh', '')}. {ng.get('mo_ta', '')}. M√£ ng√†nh: {ng.get('ma_nganh', '')}.{alias_text}"
        texts.append(text)
        ids.append(idx)
    embeddings = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)
    embeddings = np.array(embeddings, dtype=np.float32)
    if embeddings.ndim == 1:
        embeddings = embeddings.reshape(1, -1)
    if embeddings.shape[0] == 0:
        raise ValueError("Kh√¥ng c√≥ d·ªØ li·ªáu embedding ƒë·ªÉ t·∫°o FAISS index.")
    # FAISS expects 2D float32 array, linter warning here is a false positive
    print("Shape of embeddings to add to FAISS:", embeddings.shape)
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)  # Linter warning is a false positive for FAISS
    return embeddings, index, ids

def save_index_and_embeddings(embeddings, index, ids):
    np.save(EMBEDDING_FILE, embeddings)
    faiss.write_index(index, INDEX_FILE)
    with open(ID_FILE, 'w', encoding='utf-8') as f:
        json.dump(ids, f)

def load_index_and_embeddings():
    embeddings = np.load(EMBEDDING_FILE)
    index = faiss.read_index(INDEX_FILE)
    with open(ID_FILE, 'r', encoding='utf-8') as f:
        ids = json.load(f)
    return embeddings, index, ids

def ensure_index(model, nganhs):
    if os.path.exists(EMBEDDING_FILE) and os.path.exists(INDEX_FILE) and os.path.exists(ID_FILE):
        return load_index_and_embeddings()
    embeddings, index, ids = build_embeddings_and_index(nganhs, model)
    save_index_and_embeddings(embeddings, index, ids)
    return embeddings, index, ids

def keyword_match_nganh(question, nganhs):
    q = question.lower()
    q_noaccent = remove_accents(q)
    # ∆Øu ti√™n t√¨m theo alias
    for alias, ma_nganh in ALIAS_MAP.items():
        if alias in q_noaccent:
            ng = get_nganh_by_ma(ma_nganh, nganhs)
            if ng:
                return ng
    # ∆Øu ti√™n t√¨m theo m√£ ng√†nh
    for ng in nganhs:
        if ng.get('ma_nganh') and ng.get('ma_nganh') in q:
            return ng
    # So kh·ªõp t√™n ng√†nh kh√¥ng d·∫•u
    for ng in nganhs:
        if ng.get('nganh') and remove_accents(ng.get('nganh')) in q_noaccent:
            return ng
    return None

def find_best_nganh(question, model, index, nganhs, ids, top_k=1, min_score=0.5):
    # Keyword match tr∆∞·ªõc
    ng = keyword_match_nganh(question, nganhs)
    if ng:
        return ng, 1.0
    # Semantic search n·∫øu kh√¥ng keyword match
    q_emb = model.encode([question], normalize_embeddings=True)
    # Ensure correct shape for FAISS (2D array)
    q_emb = np.array(q_emb).astype('float32')
    if q_emb.ndim == 1:
        q_emb = np.expand_dims(q_emb, 0)
    D, I = index.search(q_emb, top_k)
    best_idx = ids[I[0][0]]
    score = D[0][0]
    if score < min_score:
        return None, score
    return nganhs[best_idx], score

def extract_year(question):
    m = re.search(r'(20\d{2})', question)
    return m.group(1) if m else None

def answer(question, ng, score):
    year = extract_year(question)
    q = question.lower()
    # Ph√¢n t√≠ch xu h∆∞·ªõng ƒëi·ªÉm chu·∫©n qua c√°c nƒÉm
    if any(kw in q for kw in ["ph√¢n t√≠ch", "ƒë√°nh gi√°", "so s√°nh"]) and ("ƒëi·ªÉm chu·∫©n" in q or "ƒëi·ªÉm" in q):
        if not ng.get('diem_chuan'):
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm chu·∫©n ƒë·ªÉ ph√¢n t√≠ch."
        ds = ng['diem_chuan']
        # Gom nh√≥m theo nƒÉm, m·ªói nƒÉm l·∫•y ƒëi·ªÉm cao nh·∫•t (n·∫øu c√≥ nhi·ªÅu ph∆∞∆°ng th·ª©c)
        year_points = {}
        for d in ds:
            y = d.get('nam')
            diem = d.get('diem')
            if y is not None and diem is not None:
                if y not in year_points or diem > year_points[y]:
                    year_points[y] = diem
        if not year_points:
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm chu·∫©n ƒë·ªÉ ph√¢n t√≠ch."
        years = sorted(year_points.keys())
        points = [year_points[y] for y in years]
        # Ph√¢n t√≠ch xu h∆∞·ªõng
        trend = []
        for i in range(1, len(points)):
            if points[i] > points[i-1]:
                trend.append("tƒÉng")
            elif points[i] < points[i-1]:
                trend.append("gi·∫£m")
            else:
                trend.append("gi·ªØ nguy√™n")
        # Nh·∫≠n x√©t t·ªïng qu√°t
        if all(t == "tƒÉng" for t in trend):
            nhanxet = "ƒêi·ªÉm chu·∫©n c√≥ xu h∆∞·ªõng tƒÉng qua c√°c nƒÉm."
        elif all(t == "gi·∫£m" for t in trend):
            nhanxet = "ƒêi·ªÉm chu·∫©n c√≥ xu h∆∞·ªõng gi·∫£m qua c√°c nƒÉm."
        elif all(t == "gi·ªØ nguy√™n" for t in trend):
            nhanxet = "ƒêi·ªÉm chu·∫©n gi·ªØ nguy√™n qua c√°c nƒÉm."
        else:
            nhanxet = "ƒêi·ªÉm chu·∫©n bi·∫øn ƒë·ªông qua c√°c nƒÉm."
        # Chu·∫©n b·ªã b·∫£ng ƒëi·ªÉm
        lines = [f"{ng.get('nganh')} - NƒÉm {y}: {year_points[y]}" for y in years]
        return f"{'\n'.join(lines)}\nNh·∫≠n x√©t: {nhanxet}"
    # ƒêi·ªÉm chu·∫©n
    if 'ƒëi·ªÉm chu·∫©n' in q or 'ƒëi·ªÉm' in q:
        if not ng.get('diem_chuan'):
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm chu·∫©n."
        ds = [d for d in ng['diem_chuan'] if (not year or str(d.get('nam')) == year)]
        if not ds:
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ ƒëi·ªÉm chu·∫©n nƒÉm {year}."
        lines = [f"{ng.get('nganh')} - {d.get('phuong_thuc', '')} {d.get('nam', '')}: {d.get('diem', '')}" for d in ds]
        return '\n'.join(lines)
    # ƒêi·ªÉm s√†n
    if 'ƒëi·ªÉm s√†n' in q or 't·ªëi thi·ªÉu' in q:
        diem_san = [
            {'nam': tc.get('nam'), 'diem_san': tc.get('diem_toi_thieu')}
            for tc in ng.get('tieu_chi_xet_tuyen', []) if tc.get('diem_toi_thieu')
        ]
        if not diem_san:
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm s√†n."
        ds = [d for d in diem_san if (not year or str(d.get('nam')) == year)]
        if not ds:
            return f"{ng.get('nganh')}: Kh√¥ng c√≥ ƒëi·ªÉm s√†n nƒÉm {year}."
        lines = [f"{ng.get('nganh')} - NƒÉm {d.get('nam', '')}: {d.get('diem_san', '')}" for d in ds]
        return '\n'.join(lines)
    # Ch·ªâ ti√™u
    if 'ch·ªâ ti√™u' in q:
        chi_tieu = None
        if ng.get('chi_tieu'):
            chi_tieu = ng['chi_tieu']
        elif ng.get('mo_ta'):
            m = re.search(r'Ch·ªâ ti√™u: ?(\d+)', ng['mo_ta'])
            chi_tieu = m.group(1) if m else None
        return f"{ng.get('nganh')}: Ch·ªâ ti√™u {chi_tieu if chi_tieu else 'kh√¥ng r√µ'} sinh vi√™n."
    # Khoa
    if 'khoa' in q:
        return f"{ng.get('nganh')}: {ng.get('khoa') if ng.get('khoa') else 'Kh√¥ng r√µ khoa.'}"
    # T·ªïng h·ª£p
    if ng.get('diem_chuan'):
        newest = ng['diem_chuan'][-1]
        chi_tieu = ng.get('chi_tieu')
        if not chi_tieu and ng.get('mo_ta'):
            m = re.search(r'Ch·ªâ ti√™u: ?(\d+)', ng['mo_ta'])
            chi_tieu = m.group(1) if m else None
        return f"{ng.get('nganh')} (M√£: {ng.get('ma_nganh')})\nKhoa: {ng.get('khoa')}\nCh·ªâ ti√™u: {chi_tieu if chi_tieu else 'kh√¥ng r√µ'}\nƒêi·ªÉm chu·∫©n m·ªõi nh·∫•t: {newest.get('diem', '')} ({newest.get('nam', '')})"
    return f"{ng.get('nganh')} (M√£: {ng.get('ma_nganh')})\nKhoa: {ng.get('khoa')}\nCh·ªâ ti√™u: {ng.get('chi_tieu') if ng.get('chi_tieu') else 'kh√¥ng r√µ'}"

# H√†m sinh c√¢u tr·∫£ l·ªùi b·∫±ng LLM (Ollama local)
def generate_answer_with_llm(question, top_nganhs, history=None):
    # Chu·∫©n b·ªã context ng√†nh (system message)
    context = "\n".join([
        f"{ng.get('nganh', '')} (M√£: {ng.get('ma_nganh', '')}): {ng.get('mo_ta', '')} | ƒêi·ªÉm chu·∫©n: {ng.get('diem_chuan', '') if ng.get('diem_chuan') else 'Kh√¥ng c√≥'}"
        for ng in top_nganhs
    ])
    messages = []
    # Th√™m system/context v√†o ƒë·∫ßu (ch·ªâ 1 l·∫ßn)
    messages.append({"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω tuy·ªÉn sinh ƒë·∫°i h·ªçc, h√£y tr·∫£ l·ªùi NG·∫ÆN G·ªåN, CH√çNH X√ÅC b·∫±ng ti·∫øng Vi·ªát, ch·ªâ d·ª±a tr√™n th√¥ng tin context b√™n d∆∞·ªõi. N·∫øu context kh√¥ng c√≥ th√¥ng tin, h√£y tr·∫£ l·ªùi 'Kh√¥ng c√≥ d·ªØ li·ªáu'.\n\nContext:\n" + context})
    # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i (user/assistant)
    if history:
        for turn in history[-10:]:
            messages.append({"role": "user", "content": turn["question"]})
            messages.append({"role": "assistant", "content": turn["answer"]})
    # N·∫øu c√¢u h·ªèi m·ªõi qu√° ng·∫Øn ho·∫∑c m∆° h·ªì, n·ªëi th√™m c√¢u h·ªèi tr∆∞·ªõc v√†o prompt
    def is_vague(q):
        keywords = ["ng√†nh", "ƒëi·ªÉm", "m√£ ng√†nh", "t√™n ng√†nh", "khoa", "ch·ªâ ti√™u", "nƒÉm", "m√¥ t·∫£"]
        return len(q.strip()) < 10 or not any(k in q.lower() for k in keywords)
    if history and is_vague(question):
        prev_q = history[-1]["question"]
        prev_a = history[-1]["answer"]
        user_prompt = f"C√¢u h·ªèi tr∆∞·ªõc: {prev_q}\nTr·∫£ l·ªùi tr∆∞·ªõc: {prev_a}\nC√¢u h·ªèi hi·ªán t·∫°i: {question}"
    else:
        user_prompt = question
    messages.append({"role": "user", "content": user_prompt})
    try:
        requests.get("http://localhost:11434/", timeout=3)
    except Exception:
        return "[LLM ERROR] Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama server (http://localhost:11434). B·∫°n h√£y ch·∫°y l·ªánh 'ollama serve' trong terminal tr∆∞·ªõc khi h·ªèi chatbot."
    try:
        response = requests.post(
            "http://localhost:11434/api/chat",
            json={
                "model": "mistral",
                "messages": messages,
                "stream": False
            },
            timeout=120
        )
        data = response.json()
        return data.get("message", {}).get("content", "").strip()
    except Exception as e:
        return f"[LLM ERROR] {e}"

def print_help():
    print("""
H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG CHATBOT:
- H·ªèi v·ªÅ ƒëi·ªÉm chu·∫©n, ch·ªâ ti√™u, khoa, m√¥ t·∫£ c√°c ng√†nh (VD: ƒëi·ªÉm chu·∫©n cntt 2022)
- C√≥ th·ªÉ h·ªèi ti·∫øp t·ª•c, chatbot s·∫Ω nh·ªõ ng·ªØ c·∫£nh c√°c c√¢u tr∆∞·ªõc
- L·ªánh ƒë·∫∑c bi·ªát:
  + reset: X√≥a to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i
  + help: Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n√†y
  + exit: Tho√°t chatbot
""")

# S·ª≠a h√†m main ƒë·ªÉ d√πng RAG

def main():
    print("ü§ñ Chatbot Tuy·ªÉn Sinh (Semantic Search + RAG LLM) - T√¨m ng√†nh th√¥ng minh!")
    print("=" * 60)
    print("G√µ 'help' ƒë·ªÉ xem h∆∞·ªõng d·∫´n.")
    model = SentenceTransformer(MODEL_NAME)
    nganhs = load_nganh_data()
    embeddings, index, ids = ensure_index(model, nganhs)
    history = []  # L∆∞u l·ªãch s·ª≠ chat
    while True:
        try:
            question = input("\n‚ùì C√¢u h·ªèi (ho·∫∑c 'exit', 'reset', 'help'): ").strip()
            q_lower = question.lower().strip()
            # X·ª≠ l√Ω l·ªánh ƒë·∫∑c bi·ªát NGAY SAU KHI NH·∫¨N INPUT
            if q_lower == 'exit':
                print("üëã T·∫°m bi·ªát!")
                break
            if q_lower == 'reset':
                history = []
                print("üîÑ ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
                continue
            if q_lower == 'help':
                print_help()
                continue
            if not question:
                print("üí° Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")
                continue
            # N·∫øu l√† c√¢u ƒë·∫ßu ti√™n m√† m∆° h·ªì, y√™u c·∫ßu nh·∫≠p r√µ h∆°n
            def is_vague(q):
                keywords = ["ng√†nh", "ƒëi·ªÉm", "m√£ ng√†nh", "t√™n ng√†nh", "khoa", "ch·ªâ ti√™u", "nƒÉm", "m√¥ t·∫£"]
                return len(q.strip()) < 10 or not any(k in q.lower() for k in keywords)
            if not history and is_vague(question):
                print("ü§ñ B·∫°n vui l√≤ng nh·∫≠p r√µ h∆°n t√™n ng√†nh, m√£ ng√†nh, nƒÉm ho·∫∑c m√¥ t·∫£ chi ti·∫øt h∆°n!")
                continue
            # Tr·∫£ l·ªùi t·ª´ d·ªØ li·ªáu n·∫øu t√¨m ƒë∆∞·ª£c ng√†nh ph√π h·ª£p
            ng, score = find_best_nganh(question, model, index, nganhs, ids, top_k=1, min_score=0.5)
            if ng:
                ans = answer(question, ng, score)
                print(f"\nü§ñ {ans}")
                history.append({"question": question, "answer": ans})
                continue
            # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c ng√†nh ph√π h·ª£p, fallback sang LLM v·ªõi top-k ng√†nh g·∫ßn nh·∫•t
            q_emb = model.encode([question], normalize_embeddings=True)
            q_emb = np.array(q_emb).astype('float32')
            if q_emb.ndim == 1:
                q_emb = np.expand_dims(q_emb, 0)
            D, I = index.search(q_emb, TOP_K)
            top_nganhs = [nganhs[ids[i]] for i in I[0]]
            if not top_nganhs or D[0][0] < 0.5:
                print("\nü§ñ Xin l·ªói, t√¥i ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n vui l√≤ng nh·∫≠p r√µ h∆°n t√™n ng√†nh, m√£ ng√†nh ho·∫∑c m√¥ t·∫£ chi ti·∫øt h∆°n?")
                continue
            llm_answer = generate_answer_with_llm(question, top_nganhs, history)
            if llm_answer:
                print(f"\nü§ñ [RAG LLM] {llm_answer}")
                history.append({"question": question, "answer": llm_answer})
            else:
                ng = top_nganhs[0]
                score = D[0][0]
                ans = answer(question, ng, score)
                print(f"\n[Score: {score:.3f}] {ans}")
                history.append({"question": question, "answer": ans})
        except KeyboardInterrupt:
            print("\nüëã T·∫°m bi·ªát!")
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}")
            print("üí° Th·ª≠ c√¢u h·ªèi kh√°c")

if __name__ == "__main__":
    main() 